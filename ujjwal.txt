THE NEW ONE


Final backend architecture (MVP, LAN-only, WebSockets, Docker job execution)
Components

Coordinator (Manager laptop)

Service: gridx-coordinator

Responsibilities:

Accept workers over WebSockets.

Accept job submissions (HTTP or WS).

Maintain worker registry + job queue.

Assign jobs to workers (simple FIFO + first-idle).

Receive logs/results and store them.

State (in-memory for MVP):

workers (connected, capabilities, status)

job_queue (FIFO)

jobs (status, assigned worker, timestamps, results)

Optional persistence (still simple): SQLite file for jobs table (not required).

Worker Agent (Student laptop; one per device)

Service: gridx-worker

Responsibilities:

Connect to coordinator via WebSocket.

Advertise capabilities (CPU cores, GPU yes/no).

Receive assigned job.

Run the job inside a Docker container with strict sandbox flags.

Stream logs to coordinator and return result (exit code + stdout/stderr + artifact metadata).

Worker local dependencies:

Docker Engine installed (and NVIDIA container runtime if GPU jobs are demoed).

Job Runner Image(s) (Docker)

Images: gridx-job-python, optionally gridx-job-gpu later

Responsibilities:

Provide a controlled runtime to execute a whitelisted job type.

Read job payload from a mounted/created temp file.

Write outputs to a temp output directory (which is later collected by worker).

Network & communication

Topology: all devices on same LAN

Protocols:

Worker ↔ Coordinator: WebSocket (primary control plane)

Client ↔ Coordinator: HTTP (simplest) for submit job, list jobs, get result
(You can also do client submit over WS; HTTP is simpler for demo tooling.)

Ports (example)

gridx-coordinator

8080/tcp WebSocket: /ws/worker

8081/tcp HTTP API: /jobs, /jobs/:id, /workers

gridx-worker

no inbound ports required (outbound WS to coordinator)

Coordinator services (internal modules)

WS Gateway

Auth: shared demo token (header or first message)

Tracks heartbeats (ping/pong), last_seen timestamps.

Scheduler

FIFO queue

Dispatch rule:

If job.requires_gpu == true, pick first idle worker with caps.gpu=true

Else pick first idle worker

Failure handling:

Worker disconnects while running → mark job failed and requeue (or failed only; your choice).

Job Registry

Job states: queued → assigned → running → completed|failed|canceled

Stores:

job_id, kind, payload_hash, requires_gpu, resource_limits, assigned_worker

stdout/stderr (or last N lines) and artifact list (optional)

Worker job execution (Docker-based)
Job lifecycle on worker

Receive {job_id, kind, payload, limits} from coordinator.

Validate kind is whitelisted (e.g., python only).

Materialize payload into a temp working dir on worker:

/var/lib/gridx/jobs/<job_id>/input/

/var/lib/gridx/jobs/<job_id>/output/

Run docker container with:

No host file access except that job’s temp dir

Hard resource limits

Non-root user

Read-only filesystem

No network (recommended for demo)

Docker run contract

Mounted paths:

/work/input (read-only)

/work/output (read-write)

Entrypoint in image reads /work/input/task.json and executes.

Recommended sandbox flags (MVP “final”)

--read-only

--cap-drop=ALL

--security-opt=no-new-privileges:true

--pids-limit=256

--cpus=<job.limit.cpus>

--memory=<job.limit.memory>

--network=none

--user 1000:1000

--tmpfs /tmp:rw,size=256m

-v <jobdir>/input:/work/input:ro

-v <jobdir>/output:/work/output:rw

If GPU demo is needed later:

add --gpus all and ensure NVIDIA container runtime.

Job types (keep minimal and safe)

For MVP, define one job type:

kind = "python"

payload contains:

main_py (base64 string) OR script_text

optional args

runner does:

write script to /tmp/main.py

execute python /tmp/main.py

capture stdout/stderr

optionally write outputs into /work/output

Do not accept arbitrary shell commands in MVP.

Message contracts (final)
Worker → Coordinator (WS)

hello: {type:"hello", worker_id, caps:{cpu_cores,gpu}, token}

heartbeat: {type:"hb", worker_id, ts}

job_started: {type:"job_started", job_id, worker_id}

job_log: {type:"job_log", job_id, line}

job_result: {type:"job_result", job_id, exit_code, stdout, stderr, artifacts:[{name,size,sha256}]}

Coordinator → Worker (WS)

assign_job: {type:"assign_job", job:{job_id, kind, payload, limits}}

cancel_job: {type:"cancel_job", job_id}

Client → Coordinator (HTTP)

POST /jobs → returns job_id

GET /jobs/:id → status + result if done

GET /workers → list connected workers + status

Deployment (Docker Compose)
Coordinator machine

docker compose up coordinator

Exposes 8080/8081 on LAN.

Worker machines

Either run worker as:

a small binary/script directly on host (simplest), OR

a container with access to docker socket (still common for demos):

mount /var/run/docker.sock:/var/run/docker.sock

NOTE: this gives the worker container control over host docker (acceptable for MVP, but mention in “known limitations”).

Trust boundaries & MVP security stance

Host private files protection: achieved by never mounting host directories into job containers; only mount per-job temp dir created by worker.

Network isolation: --network=none prevents jobs from scanning LAN.

Privilege isolation: drop caps + non-root + no-new-privileges.

Known limitation (explicit): Docker is not a perfect VM boundary; for a stronger story, run the worker agent inside a VM on each provider device.

Data flow summary

Worker connects to coordinator (WS) → registers caps.

Client submits job to coordinator (HTTP).

Coordinator enqueues → scheduler assigns to idle worker.

Worker runs Docker job container with sandbox flags.

Worker streams logs → coordinator.

Worker sends result → coordinator stores and exposes via GET /jobs/:id.

This is the “final backend architecture” for a working LAN prototype with Docker-based sandboxed job execution and WebSocket control plane